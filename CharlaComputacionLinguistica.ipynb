{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Plan de la charla\n",
    "\n",
    "#### Temático\n",
    "\n",
    "* Modelos de embeddings de palabras\n",
    "* Propiedades de los embeddings\n",
    "* Traducción no-supervisada\n",
    "* Traducción supervisada  \n",
    "\n",
    "#### General\n",
    "\n",
    "* Un viaje a través de distintas áreas de procesamiento de lenguaje natural y ciencia de datos en general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Qué asumimos en la charla\n",
    "\n",
    "Asumimos que no se asustan con lo siguiente:\n",
    "* algunas cosas básicas de probabilidad (distribuciones, probabilidades condicionales)\n",
    "* algunas cosas de álgebra (espacios métricos)\n",
    "* aprendizaje supervisado y no-supervisado\n",
    "* representaciones como one-hot encoding o n-gramas\n",
    "* [redes neuronales](https://www.youtube.com/watch?v=aircAruvnKk) (perceptrones multicapa)\n",
    "\n",
    "Lo siguiente no es pre-requisito para entender, pero en el la charla asumimos que existen y funcionan:\n",
    "* Si definimos una función objetivo, hay formas de encontrar parámetros que la optimicen sobre un set de datos (__entrenamiento de un modelo__)\n",
    "* Si definimos una topología de una red neuronal, es lo mismo que definir una función objetivo, y la podemos [entrenar](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&t=0s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "* Nos referimos a *embeddings* cuando a cada objeto le asignamos un vector.\n",
    "* En NLP/U, un objeto puede ser un documento, una frase o una palabra, por ejemplo\n",
    "* La idea de usar vectores es conseguir alguna propiedad en ese espacio que nos dé información de los objetos;\n",
    " + por ejemplo, que \n",
    " \n",
    " $$||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Brasil})|| > ||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Peretti})||$$\n",
    " \n",
    " \n",
    "* Generalmente es una tarea no-supervisada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embeddings de palabras  \n",
    "\n",
    "* En este caso queremos asignarle a cada palabra un vector, de manera que palabras similares en semántica queden cercas en ese espacio.\n",
    "* Como es una tarea no-supervisada, no tenemos explícitamente una referencia de las palabras similares que queremos que queden juntas.\n",
    "* Definimos un objetivo parecido entonces: optimizar la predicción del contexto a partir de una palabra.\n",
    "* Todos los modelos tienen generalmente un hiperparámetro $c$ que es la ventana de palabras a ver.\n",
    "* Si dos palabras son semánticamente similares, pueden estar en contextos similares, por lo que deberían quedar cerca en el espacio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### La hipótesis distribucional\n",
    "Conocerás una palabra por la compañía que lleva\n",
    "\n",
    "#### Por qué esto  es una buena idea\n",
    "Sea $\\psi(p_1, p_2)$ la similaridad entre las palabras $p_1$ y $p_2$. \n",
    "* ¿Cómo se define la similaridad entre palabras? \n",
    "* ¿Cómo se define la similaridad entre vectores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Un modelo sencillo: la matriz de coocurrencias\n",
    "\n",
    "* Como en esencia queremos codificar los contextos, podemos simplemente tener una matriz cuadrada donde cada fila y columna refiere a una palabra de nuestro vocabulario.\n",
    "* Los elementos de la matriz son la cantidad de coocurrencias de la palabra $i$ con la palabra $j$.\n",
    "* En definitiva, intentamos predecir la palabra actual en base al contexto (implícitamente).\n",
    "* El modelo funciona más o menos bien pero la matriz se hace impracticable en *vocabularios* grandes:\n",
    "    * La edición 2014 del DRAE tiene 93111 palabras\n",
    "    * Eso serían alrededor de 17.35 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otros modelos basados en la matriz de coocurrencias\n",
    "* Principalmente con el objetivo de reducir el tamaño de la matriz:\n",
    " + SVD (LSA)\n",
    " + otras factorizaciones (non-negative, etc.)\n",
    " + por proyecciones aleatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CBOW y Skip-gram\n",
    "\n",
    "* Modelos no supervisados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CBOW\n",
    "* La matriz de coocurrencias tiene el objetivo implícito de predecir la palabra a partir del contexto.\n",
    "* Una forma explícita y reducida de hacer esto es usar el Continuous Bag of Words Model (CBOW)\n",
    "![CBOW architecture](images/cbow_architecture.png)\n",
    "* A partir de un *one-hot encoding* por $W$ obtenemos el embedding y luego aplicamos la otra capa de la red (softmax(W*h))\n",
    "* El objetivo es predecir el one-hot encoding de $w(t)$ a partir de $w(t\\pm j)$\n",
    "* demo: https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Skip-Gram\n",
    "\n",
    "Si CBOW predice la palabra a partir del contexto, SG predice el contexto a partir de la palabra:\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p(w_{t+j}|w_t)$$ \n",
    "![CBOW vs Skip Gram](images/cbow_vs_skipgram.png)\n",
    "* Codifica explícitamente el orden de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FastText\n",
    "\n",
    "* Se incluyen representaciones de n-gramas además de las palabras\n",
    "    * Preservar el significado de palabras que son parte de otras\n",
    "    * Capturar el significado de pre/posfijos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación de los espacios: tSNE\n",
    "* Algoritmo no lineal de reducción de dimensiones\n",
    "Dado un espacio vectorial $D$ de gran dimensionalidad y un espacio de menor dimensionalidad $d$\n",
    "* Puntos cercanos en $D$ quedan juntos en $d$ con gran probabilidad\n",
    "* Puntos lejanos en $D$ quedan lejos en $d$ con gran probabilidad\n",
    "\n",
    "En esencia, intentamos maximizar que la probabilidad de ser vecinos en el espacio original sea similar a la probabilidad de ser vecinos en el espacio reducido:\n",
    "$$C = \\sum_i \\mathrm{KL}(P_i|Q_i)$$\n",
    "$P_i$ es la distribución de vecinos del punto $i$ en el espacio original; $Q_i$ la análoga en el espacio reducido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Podemos utilizar tSNE para visualizar los embeddings!\n",
    "* Aplicamos tSNE sobre un set reducido de palabras para ver cómo agrupa las palabras un algoritmo de embedding de palabras.\n",
    "\n",
    "![Ejemplo de tSNE](images/tsne_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Clustering de los embeddings\n",
    "\n",
    "Podemos utilizar un algoritmo de clustering sobre los embeddings y ver qué resulta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propiedades de los embeddings\n",
    "\n",
    "* Encontramos entonces que logramos grupos de **palabras cercanas que cumplen funciones similares**. Por ejemplo, los verbos están cerca de verbos y los sustantivos están cerca de sustantivos. Esto tiene sentido desde la hipótesis distribucional.\n",
    "* Se encontró que las relaciones tenían vectores similares:\n",
    "$$\\phi(galletitas)-\\phi(galletita) \\approx \\phi(pitusas) - \\phi(pitusa)$$\n",
    "* Entonces podemos armar analogías! Podemos resolver consultas:\n",
    "$$\\phi(galletitas)-\\phi(galletita) + \\phi(pitusa)\\approx \\phi(pitusas)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Esto aplica para las siguientes regularidades:\n",
    "![Regularidades sencillas](images/regularities_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pero además..\n",
    "\n",
    "No sólo hay relaciones sintácticas o de concordancia, también hay relaciones semánticas:\n",
    "![Regularidades semánticas](images/regularities_semantics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LSA Dimensionality](images/LSAgraph.jpg)\n",
    "\n",
    "* Pocos parámetros $\\to$ underfitting\n",
    "* Muchos parámetros $\\to$ dimension como factor de regularización para la optimización \n",
    "* Va en contra del lema de Johnson-Lindenstrauss\n",
    "    * n = d = 100000\n",
    "    * fijamos $\\epsilon$ = 0.8 (tasa de error alta)\n",
    "    * k $\\approx$ 308\n",
    "    * A mayor $k$, menor $\\epsilon$\n",
    "        * En la práctica se observa _overfitting_ a mayor $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traducción no-supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Se observa que los embeddings de distintos idiomas tienen estructuras similares.\n",
    "* Es decir, los vecinos de pares de traducción tienden a ser otros pares de traducciones.\n",
    "* Observación: la función objetivo de generación de embeddings es invariante a rotación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![embedding rotation](images/embedding_rotation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si conocemos algunos pares de traducción (*anchor points*), podemos obtener una rotación tal que casi coincidan:\n",
    "$$W^* = \\mathrm{argmin}_W (||WX-Y||_F)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una vez que tenemos esto, podemos obtener traducciones de palabras que no estaban en la tabla conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Podemos obtener $W^*$ sin puntos ancla a priori? [5]\n",
    "* Aprendizaje adversario (diferenciar entre $WX$ y $Y$)\n",
    "* Algoritmo de Procrustes en base a pares de traducción estimados con las palabras más comunes\n",
    "\n",
    "Y luego podemos usar una métrica distinta para la similaridad (CSLS, *cross domain similarity local scaling*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué queremos hacer con CSLS?  \n",
    "**problema**: en cada idioma hay palabras que son muy cercanas a otras palabras, y palabras bastante aisladas  \n",
    "**intuición**: tenemos que reducir la influencia de las palabras muy centrales en el cálculo de similaridad  \n",
    "**solución**: cuando calculamos la similaridad, podemos restar la similaridad media de una palabra a sus k-vecinos más cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Definimos $\\mathcal{N}_T(Wx_s)$ como los k-palabras más cercanas del idioma objetivo a la rotación de la palabra del idioma base ($Wx_s$).  \n",
    "* $\\mathcal{N}_S(y_t)$ es lo mismo: nos daría las palabras del idioma base que al rotarse son los k-vecinos de $y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $r_T(Wx_s) = \\frac{1}{K} \\sum_{y_t\\in \\mathcal{N}_T(Wx_s)} \\cos(Wx_s, y_t)$  \n",
    "* $r_S(y_t) = \\frac{1}{K} \\sum_{Wx_s\\in \\mathcal{N}_S(y_t)} \\cos(Wx_s, y_t)$  \n",
    "y finalmente:\n",
    "$$\\mathrm{CSLS}(Wx_s, y_t) = 2\\cos(Wx_s, y_t) - r_T(Wx_s) - r_S(y_t)$$\n",
    "(donde $\\cos(\\cdot,\\cdot)$ es la similaridad coseno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¡Listo!** (por ahora)  \n",
    "¿Qué hicimos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Armamos un modelo que puede traducir palabras:\n",
    "* Generamos los embeddings con algún algoritmo (word2vec, GloVe, fasttext)\n",
    "* Transformamos uno de los espacios para que coincida lo más posible con otro\n",
    "* Definimos una función de similaridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Y ahora?\n",
    "Vamos a armar un modelo, que se construye partiendo de éste, para traducir oraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interludio: espacios latentes, encoders y decoders\n",
    "\n",
    "* Podemos generar un embedding de una oración a través de embeddings de palabras y [red neuronal recurrente](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (en particular, una LSTM).\n",
    "* Una red recurrente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![rnn unrolled](images/RNN-unrolled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En este caso, usamos una red Long Short Term Memory -- una red recurrente diseñada para manejar el problema de las dependencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¿Cómo las usamos?**\n",
    "\n",
    "De ida: (*encoding*) Los $x_t$ son los embeddings de las palabras de la oración y el conjunto de los $h_t$ termina siendo nuestro vector que representa la oración.  \n",
    "De vuelta: (*decoding*) Otra red toma los $h_t$ y produce unos $x'_t$ que esperamos que sean similares a los $x_t$ originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esto es un autoencoder: tenemos un espacio latente de representación al cual podemos ir y volver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Queremos que sea robusto y que no memorice las cosas, por lo cual agregamos ruido a las oraciones. Tenemos dos tipos de ruido:\n",
    "* reordenar levemente las palabras\n",
    "* sacar una palabra con cierta probabilidad\n",
    "a la oración con ruido la llamamos $C(x)$ y pedimos que $d(e(x)) \\approx x \\approx d(e(C(x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Visualmente:  \n",
    "![denoising autoencoder](./images/denoising_autoencoder_latent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Por ahora, parecería que tenemos un encoder+decoder por cada idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a menos que.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* hagamos que los encoders y decoders compartan todos los pesos de las matrices, y agregamos un parámetro que marque el idioma!\n",
    "* ¿qué generamos con esto? un mismo espacio latente para ambos idiomas, y un autoencoder que según un parámetro los mueve.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una vez que tenemos esto, tenemos los siguientes parámetros para optimizar:\n",
    "* $\\theta_{enc}$, los parámetros del encoder\n",
    "* $\\theta_{dec}$, los parámetros del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El objetivo de este modelo tiene tres partes:\n",
    "\n",
    "* que el autoencoder, dentro del mismo idioma, decodifique las versiones codificadas con ruido de la oración en una posición cerca a la versión original de la oración.  \n",
    "* que el autoencoder, con idioma destino distinto al de origen, decodifique la traducción de la oración inicial (con un poco de ruido).\n",
    "* que el codificador mapee los dos idiomas a la misma región del espacio latente (_alineamiento_!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Oiga:  ¿y la traducción?**\n",
    "\n",
    "![umt algorithm](images/umt_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Referencias\n",
    "[1] Mikolov: Distributed Representations of Words and Phrases and their Compositionality https://arxiv.org/pdf/1310.4546.pdf   \n",
    "[2] Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space https://arxiv.org/pdf/1301.3781.pdf  \n",
    "[3] Rong: word2vec Parameter Learning Explained https://arxiv.org/pdf/1411.2738.pdf  \n",
    "[4] Mikolov: Linguistic Regularities in Continuous Space Word Representations https://www.aclweb.org/anthology/N13-1090  \n",
    "[5] Conneau, Lample: [Word translation without parallel data](https://arxiv.org/pdf/1710.04087.pdf)  \n",
    "[6] Lample, Conneau: [Unsupervised Machine Translation using monolingual corpora only](https://arxiv.org/pdf/1711.00043.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
