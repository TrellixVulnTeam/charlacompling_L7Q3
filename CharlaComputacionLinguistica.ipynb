{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de la charla\n",
    "\n",
    "#### Temático\n",
    "\n",
    "* Modelos de embeddings de palabras\n",
    "* Propiedades de los embeddings\n",
    "* Traducción no-supervisada\n",
    "* Traducción supervisada  \n",
    "\n",
    "#### General\n",
    "\n",
    "* Un viaje a través de distintas áreas de procesamiento de lenguaje natural y ciencia de datos en general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "* En NLP nos referimos a *embeddings* cuando a cada objeto le asignamos un vector.\n",
    "* Un objeto puede ser un documento, una frase o una palabra.\n",
    "* La idea de usar vectores es conseguir alguna propiedad en ese espacio que nos dé información de los objetos;\n",
    " + por ejemplo, que $$||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Brasil})|| > ||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Peretti})||$$\n",
    "* Generalmente es una tarea no-supervisada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings de palabras  \n",
    "\n",
    "* En este caso queremos asignarle a cada palabra un vector, de manera que palabras similares en semántica queden cercas en ese espacio.\n",
    "* Como es una tarea no-supervisada, no tenemos explícitamente una referencia de las palabras similares que queremos que queden juntas.\n",
    "* Definimos un objetivo parecido entonces: optimizar la predicción del contexto a partir de una palabra[1]:\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p(w_{t+j}|w_t)$$ \n",
    "* Todos los modelos tienen generalmente un hiperparámetro $c$ que es la ventana de palabras a ver.\n",
    "* Si dos palabras son semánticamente similares, pueden estar en contextos similares, por lo que deberían quedar cerca en el espacio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un modelo sencillo: la matriz de coocurrencias\n",
    "\n",
    "* Como en esencia queremos codificar los contextos, podemos simplemente tener una matriz cuadrada donde cada fila y columna refiere a una palabra.\n",
    "* Los elementos de la matriz son la cantidad de coocurrencias de la palabra $i$ con la palabra $j$.\n",
    "* El modelo funciona más o menos bien pero la matriz se hace impracticable en *corpus* grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros modelos basados en la matriz de coocurrencias\n",
    "* Principalmente con el objetivo de reducir el tamaño de la matriz:\n",
    " + SVD\n",
    " + otras factorizaciones (non-negative, etc.)\n",
    " + por proyecciones aleatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram y word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propiedades interesantes de los espacios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de los espacios: tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "[1] Mikolov: Distributed Representations of Words and Phrases and their Compositionality https://arxiv.org/pdf/1310.4546.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
