{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de la charla\n",
    "\n",
    "#### Temático\n",
    "\n",
    "* Modelos de embeddings de palabras\n",
    "* Propiedades de los embeddings\n",
    "* Traducción no-supervisada\n",
    "* Traducción supervisada  \n",
    "\n",
    "#### General\n",
    "\n",
    "* Un viaje a través de distintas áreas de procesamiento de lenguaje natural y ciencia de datos en general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "* Nos referimos a *embeddings* cuando a cada objeto le asignamos un vector.\n",
    "* En NLP/U, un objeto puede ser un documento, una frase o una palabra, por ejemplo\n",
    "* La idea de usar vectores es conseguir alguna propiedad en ese espacio que nos dé información de los objetos;\n",
    " + por ejemplo, que \n",
    " \n",
    " $$||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Brasil})|| > ||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Peretti})||$$\n",
    " \n",
    " \n",
    "* Generalmente es una tarea no-supervisada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings de palabras  \n",
    "\n",
    "* En este caso queremos asignarle a cada palabra un vector, de manera que palabras similares en semántica queden cercas en ese espacio.\n",
    "* Como es una tarea no-supervisada, no tenemos explícitamente una referencia de las palabras similares que queremos que queden juntas.\n",
    "* Definimos un objetivo parecido entonces: optimizar la predicción del contexto a partir de una palabra.\n",
    "* Todos los modelos tienen generalmente un hiperparámetro $c$ que es la ventana de palabras a ver.\n",
    "* Si dos palabras son semánticamente similares, pueden estar en contextos similares, por lo que deberían quedar cerca en el espacio.\n",
    "\n",
    "#### La hipótesis distribucional\n",
    "Conocerás una palabra por la compañía que lleva\n",
    "\n",
    "#### Por qué esto  es una buena idea\n",
    "Sea $\\psi(p_1, p_2)$ la similaridad entre las palabras $p_1$ y $p_2$. \n",
    "* ¿Cómo se define la similaridad entre palabras? \n",
    "* ¿Cómo se define la similaridad entre vectores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un modelo sencillo: la matriz de coocurrencias\n",
    "\n",
    "* Como en esencia queremos codificar los contextos, podemos simplemente tener una matriz cuadrada donde cada fila y columna refiere a una palabra de nuestro vocabulario.\n",
    "* Los elementos de la matriz son la cantidad de coocurrencias de la palabra $i$ con la palabra $j$.\n",
    "* En definitiva, intentamos predecir la palabra actual en base al contexto (implícitamente).\n",
    "* El modelo funciona más o menos bien pero la matriz se hace impracticable en *vocabularios* grandes:\n",
    "    * La edición 2014 del DRAE tiene 93111 palabras\n",
    "    * Eso serían alrededor de 17.35 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros modelos basados en la matriz de coocurrencias\n",
    "* Principalmente con el objetivo de reducir el tamaño de la matriz:\n",
    " + SVD (LSA)\n",
    " + otras factorizaciones (non-negative, etc.)\n",
    " + por proyecciones aleatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW y Skip-gram\n",
    "\n",
    "* Modelos no supervisados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "* La matriz de coocurrencias tiene el objetivo implícito de predecir la palabra a partir del contexto.\n",
    "* Una forma explícita y reducida de hacer esto es usar el Continuous Bag of Words Model (CBOW)\n",
    "![CBOW architecture](images/cbow_architecture.png)\n",
    "* A partir de un *one-hot encoding* por $W$ obtenemos el embedding y luego aplicamos la otra capa de la red (softmax(W*h))\n",
    "* El objetivo es predecir el one-hot encoding de $w(t)$ a partir de $w(t\\pm j)$\n",
    "* demo: https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram\n",
    "\n",
    "Si CBOW predice la palabra a partir del contexto, SG predice el contexto a partir de la palabra:\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p(w_{t+j}|w_t)$$ \n",
    "![CBOW vs Skip Gram](images/cbow_vs_skipgram.png)\n",
    "* Codifica explícitamente el orden de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "\n",
    "* Se incluyen representaciones de n-gramas además de las palabras\n",
    "    * Preservar el significado de palabras que son parte de otras\n",
    "    * Capturar el significado de pre/posfijos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de los espacios: tSNE\n",
    "* Algoritmo no lineal de reducción de dimensiones\n",
    "Dado un espacio vectorial $D$ de gran dimensionalidad y un espacio de menor dimensionalidad $d$\n",
    "* Puntos cercanos en $D$ quedan juntos en $d$ con gran probabilidad\n",
    "* Puntos lejanos en $D$ quedan lejos en $d$ con gran probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades de los embeddings\n",
    "\n",
    "![LSA Dimensionality](images/LSAgraph.jpg)\n",
    "\n",
    "* Pocos parámetros $\\to$ underfitting\n",
    "* Muchos parámetros $\\to$ dimension como factor de regularización para la optimización \n",
    "* Va en contra del lema de Johnson-Lindenstrauss\n",
    "    * n = d = 100000\n",
    "    * fijamos $\\epsilon$ = 0.8 (tasa de error alta)\n",
    "    * k $\\approx$ 308\n",
    "    * A mayor $k$, menor $\\epsilon$\n",
    "        * En la práctica se observa _overfitting_ a mayor $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción no-supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "[1] Mikolov: Distributed Representations of Words and Phrases and their Compositionality https://arxiv.org/pdf/1310.4546.pdf   \n",
    "[2] Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space https://arxiv.org/pdf/1301.3781.pdf  \n",
    "[3] Rong: word2vec Parameter Learning Explained https://arxiv.org/pdf/1411.2738.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
