{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Plan de la charla\n",
    "\n",
    "#### Temático\n",
    "\n",
    "* Modelos de embeddings de palabras\n",
    "* Propiedades de los embeddings\n",
    "* Traducción no-supervisada\n",
    "* Traducción supervisada  \n",
    "\n",
    "#### General\n",
    "\n",
    "* Un viaje a través de distintas áreas de procesamiento de lenguaje natural y ciencia de datos en general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Qué asumimos en la charla\n",
    "\n",
    "Asumimos que no se asustan con lo siguiente:\n",
    "* algunas cosas básicas de probabilidad (distribuciones, probabilidades condicionales)\n",
    "* algunas cosas de álgebra (espacios métricos)\n",
    "* aprendizaje supervisado y no-supervisado\n",
    "* representaciones como one-hot encoding o n-gramas\n",
    "* [redes neuronales](https://www.youtube.com/watch?v=aircAruvnKk) (perceptrones multicapa)\n",
    "\n",
    "Lo siguiente no es pre-requisito para entender, pero en el la charla asumimos que existen y funcionan:\n",
    "* Si definimos una función objetivo, hay formas de encontrar parámetros que la optimicen sobre un set de datos (__entrenamiento de un modelo__)\n",
    "* Si definimos una topología de una red neuronal, es lo mismo que definir una función objetivo, y la podemos [entrenar](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&t=0s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "* Nos referimos a *embeddings* cuando a cada objeto le asignamos un vector.\n",
    "* En NLP/U, un objeto puede ser un documento, una frase o una palabra, por ejemplo\n",
    "* La idea de usar vectores es conseguir alguna propiedad en ese espacio que nos dé información de los objetos;\n",
    " + por ejemplo, que \n",
    " \n",
    " $$||\\phi(\\mathrm{Torta}) - \\phi(\\mathrm{Bizcochuelo})|| < ||\\phi(\\mathrm{Silla}) - \\phi(\\mathrm{Rusia})||$$\n",
    " \n",
    " \n",
    "* Generalmente es una tarea no-supervisada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embeddings de palabras  \n",
    "\n",
    "* En este caso queremos asignarle a cada palabra un vector, de manera que palabras similares en semántica queden cercas en ese espacio.\n",
    "* Como es una tarea no-supervisada, no tenemos explícitamente una referencia de las palabras similares que queremos que queden juntas.\n",
    "* Definimos un objetivo parecido entonces: optimizar la predicción del contexto a partir de una palabra.\n",
    "* Todos los modelos tienen generalmente un hiperparámetro $c$ que es la ventana de palabras a ver.\n",
    "* Si dos palabras son semánticamente similares, pueden estar en contextos similares, por lo que deberían quedar cerca en el espacio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### La hipótesis distribucional\n",
    "Conocerás una palabra por la compañía que lleva\n",
    "\n",
    "#### Por qué esto  es una buena idea\n",
    "Sea $\\psi(p_1, p_2)$ la similaridad entre las palabras $p_1$ y $p_2$. \n",
    "* ¿Cómo se define la similaridad entre palabras? \n",
    "* ¿Cómo se define la similaridad entre vectores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Un modelo sencillo: la matriz de coocurrencias\n",
    "\n",
    "* Como en esencia queremos codificar los contextos, podemos simplemente tener una matriz cuadrada donde cada fila y columna refiere a una palabra de nuestro vocabulario.\n",
    "* Los elementos de la matriz son la cantidad de coocurrencias de la palabra $i$ con la palabra $j$.\n",
    "* En definitiva, intentamos predecir la palabra actual en base al contexto (implícitamente).\n",
    "* El modelo funciona más o menos bien pero la matriz se hace impracticable en *vocabularios* grandes:\n",
    "    * La edición 2014 del DRAE tiene 93111 palabras\n",
    "    * Eso sería alrededor de 17.35 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otros modelos basados en la matriz de coocurrencias\n",
    "* Principalmente con el objetivo de [reducir el tamaño de la matriz](https://youtu.be/T8tQZChniMk?t=923):\n",
    " + SVD (LSA)\n",
    " + otras factorizaciones (non-negative, etc.)\n",
    " + por proyecciones aleatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CBOW y Skip-gram\n",
    "\n",
    "* Modelos no supervisados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CBOW\n",
    "* La matriz de coocurrencias tiene el objetivo implícito de predecir la palabra a partir del contexto.\n",
    "* Una forma explícita y reducida de hacer esto es usar el Continuous Bag of Words Model (CBOW)\n",
    "![CBOW architecture](images/cbow_architecture.png)\n",
    "* A partir de un *one-hot encoding* por $W$ obtenemos el embedding y luego aplicamos la otra capa de la red (softmax(W*h))\n",
    "* El objetivo es predecir el one-hot encoding de $w(t)$ a partir de $w(t\\pm j)$\n",
    "* demo: https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Skip-Gram\n",
    "\n",
    "Si CBOW predice la palabra a partir del contexto, [SG predice el contexto a partir de la palabra](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/):\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p(w_{t+j}|w_t)$$ \n",
    "![CBOW vs Skip Gram](images/cbow_vs_skipgram.png)\n",
    "* Codifica explícitamente el orden de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [FastText](https://fasttext.cc/docs/en/support.html)\n",
    "\n",
    "* Se incluyen representaciones de n-gramas además de las palabras\n",
    "    * Preservar el significado de palabras que son parte de otras\n",
    "    * Capturar el significado de pre/posfijos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación de los espacios: tSNE\n",
    "* Algoritmo no lineal de reducción de dimensiones\n",
    "Dado un espacio vectorial $D$ de gran dimensionalidad y un espacio de menor dimensionalidad $d$\n",
    "* Puntos cercanos en $D$ quedan juntos en $d$ con gran probabilidad\n",
    "* Puntos lejanos en $D$ quedan lejos en $d$ con gran probabilidad\n",
    "\n",
    "En esencia, intentamos maximizar que la probabilidad de ser vecinos en el espacio original sea similar a la probabilidad de ser vecinos en el espacio reducido:\n",
    "$$C = \\sum_i \\mathrm{KL}(P_i|Q_i)$$\n",
    "$P_i$ es la distribución de vecinos del punto $i$ en el espacio original; $Q_i$ la análoga en el espacio reducido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Podemos utilizar tSNE para visualizar los embeddings!\n",
    "* Aplicamos tSNE sobre un set reducido de palabras para ver cómo agrupa las palabras un algoritmo de embedding de palabras.\n",
    "\n",
    "![Ejemplo de tSNE](images/tsne_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propiedades de los embeddings\n",
    "\n",
    "* [Encontramos](https://www.aclweb.org/anthology/N13-1090) entonces que logramos grupos de **palabras cercanas que cumplen funciones similares**. Por ejemplo, los verbos están cerca de verbos y los sustantivos están cerca de sustantivos. Esto tiene sentido desde la hipótesis distribucional.\n",
    "* Se encontró que las relaciones tenían vectores similares:\n",
    "$$\\phi(galletitas)-\\phi(galletita) \\approx \\phi(pitusas) - \\phi(pitusa)$$\n",
    "* Entonces podemos armar analogías! Podemos resolver consultas:\n",
    "$$\\phi(galletitas)-\\phi(galletita) + \\phi(pitusa)\\approx \\phi(pitusas)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Esto aplica para las siguientes regularidades:\n",
    "![Regularidades sencillas](images/regularities_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pero además..\n",
    "\n",
    "No sólo hay relaciones sintácticas o de concordancia, también hay relaciones semánticas:\n",
    "![Regularidades semánticas](images/regularities_semantics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LSA Dimensionality](images/LSAgraph.jpg)\n",
    "\n",
    "* Pocos parámetros $\\to$ underfitting\n",
    "* Muchos parámetros $\\to$ dimension como factor de regularización para la optimización \n",
    "* Va en contra del lema de Johnson-Lindenstrauss\n",
    "    * n = d = 100000\n",
    "    * fijamos $\\epsilon$ = 0.8 (tasa de error alta)\n",
    "    * k $\\approx$ 308\n",
    "    * A mayor $k$, menor $\\epsilon$\n",
    "        * En la práctica se observa _overfitting_ a mayor $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traducción no-supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Se [observa](https://arxiv.org/pdf/1710.04087.pdf) que los embeddings de distintos idiomas tienen estructuras similares.\n",
    "* Es decir, los vecinos de pares de traducción tienden a ser otros pares de traducciones.\n",
    "* Observación: la función objetivo de generación de embeddings es invariante a rotación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![embedding rotation](images/embedding_rotation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si conocemos algunos pares de traducción (*anchor points*), podemos [obtener una rotación](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem) tal que casi coincidan:\n",
    "$$W^* = \\mathrm{argmin}_W (||WX-Y||_F)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una vez que tenemos esto, podemos obtener traducciones de palabras que no estaban en la tabla conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Podemos obtener $W^*$ sin puntos ancla a priori? (Respuesta: [sí](https://arxiv.org/pdf/1710.04087.pdf))\n",
    "* [Aprendizaje adversario](https://skymind.ai/wiki/generative-adversarial-network-gan) (diferenciar entre $WX$ y $Y$)\n",
    "* [Algoritmo de Procrustes](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem) en base a pares de traducción estimados con las palabras más comunes\n",
    "\n",
    "Y luego podemos usar una métrica distinta para la similaridad (CSLS, *cross domain similarity local scaling*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué queremos hacer con CSLS?  \n",
    "**problema**: en cada idioma hay palabras que son muy cercanas a otras palabras, y palabras bastante aisladas  \n",
    "**intuición**: tenemos que reducir la influencia de las palabras muy centrales en el cálculo de similaridad  \n",
    "**solución**: cuando calculamos la similaridad, podemos restar la similaridad media de una palabra a sus k-vecinos más cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Definimos $\\mathcal{N}_T(Wx_s)$ como los k-palabras más cercanas del idioma objetivo a la rotación de la palabra del idioma base ($Wx_s$).  \n",
    "* $\\mathcal{N}_S(y_t)$ es lo mismo: nos daría las palabras del idioma base que al rotarse son los k-vecinos de $y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $r_T(Wx_s) = \\frac{1}{K} \\sum_{y_t\\in \\mathcal{N}_T(Wx_s)} \\cos(Wx_s, y_t)$  \n",
    "* $r_S(y_t) = \\frac{1}{K} \\sum_{Wx_s\\in \\mathcal{N}_S(y_t)} \\cos(Wx_s, y_t)$  \n",
    "y finalmente:\n",
    "$$\\mathrm{CSLS}(Wx_s, y_t) = 2\\cos(Wx_s, y_t) - r_T(Wx_s) - r_S(y_t)$$\n",
    "(donde $\\cos(\\cdot,\\cdot)$ es la similaridad coseno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¡Listo!** (por ahora)  \n",
    "¿Qué hicimos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Armamos un modelo que puede traducir palabras:\n",
    "* Generamos los embeddings con algún algoritmo (word2vec, GloVe, fasttext)\n",
    "* Transformamos uno de los espacios para que coincida lo más posible con otro\n",
    "* Definimos una función de similaridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Y ahora?\n",
    "Vamos a armar un modelo, que se construye partiendo de éste, para traducir oraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interludio: espacios latentes, encoders y decoders\n",
    "\n",
    "* Podemos generar un embedding de una oración a través de embeddings de palabras y [red neuronal recurrente](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (en particular, una LSTM).\n",
    "* Una red recurrente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![rnn unrolled](images/RNN-unrolled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En este caso, usamos una red Long Short Term Memory -- una red recurrente diseñada para manejar el problema de las dependencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¿Cómo las usamos?**\n",
    "\n",
    "De ida: (*encoding*) Los $x_t$ son los embeddings de las palabras de la oración y el conjunto de los $h_t$ termina siendo nuestro vector que representa la oración.  \n",
    "De vuelta: (*decoding*) Otra red toma los $h_t$ y produce unos $x'_t$ que esperamos que sean similares a los $x_t$ originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esto es un autoencoder: tenemos un espacio latente de representación al cual podemos ir y volver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Queremos que sea robusto y que no memorice las cosas, por lo cual agregamos ruido a las oraciones. Tenemos dos tipos de ruido:\n",
    "* reordenar levemente las palabras\n",
    "* sacar una palabra con cierta probabilidad\n",
    "a la oración con ruido la llamamos $C(x)$ y pedimos que $d(e(x)) \\approx x \\approx d(e(C(x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Visualmente:  \n",
    "![denoising autoencoder](./images/denoising_autoencoder_latent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Por ahora, parecería que tenemos un encoder+decoder por cada idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a menos que.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* hagamos que los encoders y decoders compartan todos los pesos de las matrices, y agregamos un parámetro que marque el idioma!\n",
    "* ¿qué generamos con esto? un mismo espacio latente para ambos idiomas, y un autoencoder que según un parámetro los mueve.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una vez que tenemos esto, tenemos los siguientes parámetros para optimizar:\n",
    "* $\\theta_{enc}$, los parámetros del encoder\n",
    "* $\\theta_{dec}$, los parámetros del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El objetivo de este modelo tiene tres partes:\n",
    "\n",
    "* que el autoencoder, dentro del mismo idioma, decodifique las versiones codificadas con ruido de la oración en una posición cerca a la versión original de la oración.  \n",
    "* que el autoencoder, con idioma destino distinto al de origen, decodifique la traducción de la oración inicial (con un poco de ruido).\n",
    "* que el codificador mapee los dos idiomas a la misma región del espacio latente (_alineamiento_!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Oiga:  ¿y la traducción?**\n",
    "\n",
    "![umt algorithm](images/umt_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traducción supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Atención: es otro mundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* hay muchos approaches, generalmente basados en redes neuronales\n",
    "* redes neuronales recurrentes (LSTMs) y convolucionales (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* nos vamos a basar en el paper [_Attention is all you need_](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En escala grande, seguimos trabajando con arquitecturas encoder-decoder:\n",
    "![encoder-decoder](https://smerity.com/media/images/articles/2016/gnmt_arch_1_enc_dec.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tenemos un espacio latente donde cada elemento representa una oración\n",
    "* La representación latente se obtiene a partir de redes recurrentes que parten de word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_“You can’t cram the meaning of a whole %&!?ing sentence into a single %&!?ing vector!”_\n",
    "\n",
    "Ray Mooney\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* La representación latente tiene _mucha_ información, por lo cual la vamos a analizar de a partes (_mecanismo de atención_).\n",
    "* Imita lo que hacemos los humanos cuando intentamos traducir una oración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mecanismo de atención\n",
    "El [mecanismo de atención](https://skymind.ai/wiki/attention-mechanism-memory-network) es una manera de pesar los valores del vector\n",
    "![attention_mechanism](https://skymind.ai/images/wiki/attention_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(la idea es que miramos varias veces al vector, pero cada vez nos enfocamos en partes _distintas_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* formalmente, podemos pensar que tenemos una función de ranking $R$ que nos dice cuán relevante es cada elemento del vector de memoria (vector de oración en el espacio latente) para el estado actual del decoder\n",
    "* $R: \\mathrm{representacion\\ latente} \\times \\mathrm{estado\\ decoder} \\rightarrow [0,1]^n$\n",
    "* el producto elemento-a-elemento de la representación latente y $R(z_e, z_d^t)$ nos da el vector input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos ver que se pasa una [atención para cada output del decoder](https://smerity.com/articles/2016/google_nmt_arch.html), a diferencia de antes:\n",
    "![attention_in_encoder_decoder](https://smerity.com/media/images/articles/2016/gnmt_arch_attn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Un problema es que el vector latente fue generado de izquierda-a-derecha, por lo que la posición $i$-ésima no tiene información sobre la palabra $j$ si $i<j$.\n",
    "* Esto puede ser molesto para traducción entre idiomas que manejan órdenes distintos entre palabras (e.g. SVO, SOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Metemos más LSTM, [una va de izquierda a derecha y otra en sentido inverso](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks):\n",
    "![bidirectional_lstm](https://smerity.com/media/images/articles/2016/gnmt_arch_1_birnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una forma de atención es la siguiente:\n",
    "$$Attention(Q,K,V) = softmax\\left ( \\frac{QK^T}{\\sqrt{d_k}} \\right ) V$$ \n",
    "es un algoritmo de ranking:\n",
    "* Q: matriz de queries\n",
    "* K: matriz de claves\n",
    "* V: matriz de valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* El paper utiliza un método que se llama **Multi-Head Attention**\n",
    "![multi head attention](https://i.imgur.com/9bsFXZV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* proyecta de varias maneras distintas la información, aplica la función de atención, concatenamos y proyectamos\n",
    "* la idea es que cada atención distinta se fija en cosas distintas (una más en concordancia, otra en relación con el sujeto..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* el paper usa 8 proyecciones iniciales distintas (\"pares de ojos\" mirando la oración)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* el nombre del paper es *Attention is all you need*\n",
    "* ¡no usan recurrencias ni convoluciones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* es necesario una manera de representar las posiciones (que eran implícitas en las convoluciones y recurrencias)\n",
    "* le suman a los embeddings unos vectores que son de posición ([*positional encodings*](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)):\n",
    "$$\\mathrm{PE}_{pos, 2i} = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_model}}} \\right ) $$\n",
    "$$\\mathrm{PE}_{pos, 2i+1} = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_model}}} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ahora sí, la arquitectura final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![aiayn](https://i.imgur.com/s5KOShH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* vemos que hay tres usos de atención:\n",
    " + del encoder hacia el decoder (la que vimos)\n",
    " + _dentro del propio encoder_\n",
    " + _dentro del propio decoder_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* la autoatención permite reducir la complejidad del algoritmo y paralelizarlo\n",
    "* también permite aprender dependencias más largas (difícil en redes recurrentes o convolucionales!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* el paper tiene visualizaciones interesantes sobre la auto-atención:\n",
    "![self attention](https://i.imgur.com/MwLmImn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Referencias\n",
    "[1] Mikolov: Distributed Representations of Words and Phrases and their Compositionality https://arxiv.org/pdf/1310.4546.pdf   \n",
    "[2] Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space https://arxiv.org/pdf/1301.3781.pdf  \n",
    "[3] Rong: word2vec Parameter Learning Explained https://arxiv.org/pdf/1411.2738.pdf  \n",
    "[4] Mikolov: Linguistic Regularities in Continuous Space Word Representations https://www.aclweb.org/anthology/N13-1090  \n",
    "[5] Conneau, Lample: [Word translation without parallel data](https://arxiv.org/pdf/1710.04087.pdf)  \n",
    "[6] Lample, Conneau: [Unsupervised Machine Translation using monolingual corpora only](https://arxiv.org/pdf/1711.00043.pdf)  \n",
    "[7] Skymind: [Attention Mechanisms and Memory Networks](https://skymind.ai/wiki/attention-mechanism-memory-network)  \n",
    "[8] https://smerity.com/articles/2016/google_nmt_arch.html  \n",
    "[9] http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/  \n",
    "[10] Vaswani, Shazeer: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
