{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de la charla\n",
    "\n",
    "#### Temático\n",
    "\n",
    "* Modelos de embeddings de palabras\n",
    "* Propiedades de los embeddings\n",
    "* Traducción no-supervisada\n",
    "* Traducción supervisada  \n",
    "\n",
    "#### General\n",
    "\n",
    "* Un viaje a través de distintas áreas de procesamiento de lenguaje natural y ciencia de datos en general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "* En NLP nos referimos a *embeddings* cuando a cada objeto le asignamos un vector.\n",
    "* Un objeto puede ser un documento, una frase o una palabra.\n",
    "* La idea de usar vectores es conseguir alguna propiedad en ese espacio que nos dé información de los objetos;\n",
    " + por ejemplo, que $$||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Brasil})|| > ||\\phi(\\mathrm{Argentina}) - \\phi(\\mathrm{Peretti})||$$\n",
    "* Generalmente es una tarea no-supervisada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings de palabras  \n",
    "\n",
    "* En este caso queremos asignarle a cada palabra un vector, de manera que palabras similares en semántica queden cercas en ese espacio.\n",
    "* Como es una tarea no-supervisada, no tenemos explícitamente una referencia de las palabras similares que queremos que queden juntas.\n",
    "* Definimos un objetivo parecido entonces: optimizar la predicción del contexto a partir de una palabra.\n",
    "* Todos los modelos tienen generalmente un hiperparámetro $c$ que es la ventana de palabras a ver.\n",
    "* Si dos palabras son semánticamente similares, pueden estar en contextos similares, por lo que deberían quedar cerca en el espacio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un modelo sencillo: la matriz de coocurrencias\n",
    "\n",
    "* Como en esencia queremos codificar los contextos, podemos simplemente tener una matriz cuadrada donde cada fila y columna refiere a una palabra.\n",
    "* Los elementos de la matriz son la cantidad de coocurrencias de la palabra $i$ con la palabra $j$.\n",
    "* En definitiva, intentamos predecir la palabra actual en base al contexto (implícitamente).\n",
    "* El modelo funciona más o menos bien pero la matriz se hace impracticable en *corpus* grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros modelos basados en la matriz de coocurrencias\n",
    "* Principalmente con el objetivo de reducir el tamaño de la matriz:\n",
    " + SVD (LSA)\n",
    " + otras factorizaciones (non-negative, etc.)\n",
    " + por proyecciones aleatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW y Skip-gram\n",
    "* La matriz de coocurrencias tiene el objetivo implícito de predecir la palabra a partir del contexto.\n",
    "* Una forma explícita y reducida de hacer esto es usar el Continuous Bag of Words Model (CBOW)\n",
    "![CBOW architecture](images/cbow_architecture.png)\n",
    "* A partir de un *one-hot encoding* por $W$ obtenemos el embedding y luego aplicamos la otra capa de la red (softmax(W*h))\n",
    "* El objetivo es predecir el one-hot encoding de $w(t)$ a partir de $w(t\\pm j)$\n",
    "* demo: https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si CBOW predice la palabra a partir del contexto, SG predice el contexto a partir de la palabra:\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p(w_{t+j}|w_t)$$ \n",
    "![CBOW vs Skip Gram](images/cbow_vs_skipgram.png)\n",
    "* Codifica explícitamente el orden de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de los espacios: tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades de los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción no-supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "[1] Mikolov: Distributed Representations of Words and Phrases and their Compositionality https://arxiv.org/pdf/1310.4546.pdf   \n",
    "[2] Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space https://arxiv.org/pdf/1301.3781.pdf  \n",
    "[3] Rong: word2vec Parameter Learning Explained https://arxiv.org/pdf/1411.2738.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
